{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115bf14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ÌòÑÏû¨ ÏûëÏóÖ Ìè¥Îçî: c:\\Users\\ÌòÑÏÑùÏù¥\\Desktop\\second project\n",
      "‚úÖ ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÌååÏùº ÌôïÏù∏Îê®: dataset_v.02.json\n",
      "‚úÖ Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî ÌôïÏù∏Îê®: ./data/train\n",
      "üî• ViT-GPT2 ÌïôÏäµ ÏãúÏûë (Device: cuda)\n",
      "üìä Ï¥ù 1082Ïû•Ïùò Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµÏùÑ Ï§ÄÎπÑÌï©ÎãàÎã§.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ÌòÑÏÑùÏù¥\\AppData\\Local\\Temp\\ipykernel_22908\\160294857.py:142: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ÌïôÏäµ ÏãúÏûë! (Ïû†Ïãú Í∏∞Îã§Î¶¨ÏÑ∏Ïöî...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2710' max='2710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2710/2710 17:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.085900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.800500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.719800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.546800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.571300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.511800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.449800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.437200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.476500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.482300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.369500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.515800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.304200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.266100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.372400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.314100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.333000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.229100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.289200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.260300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.288500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.356600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.242700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.235400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.175600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.139600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.175200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.126700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.171800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.133100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.135900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.119800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.099400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.083800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.103700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.167800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.097600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.079900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.080500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.084100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.086400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.090600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.065900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.098500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.070200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.090300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.069900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.100700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>0.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>0.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>0.034400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>0.044100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>0.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>0.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>0.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>0.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ÌòÑÏÑùÏù¥\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'num_beams': 4}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ ÌïôÏäµ ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "# Ïù¥ ÏΩîÎìúÎ•º Î≥µÏÇ¨Ìï¥ÏÑú ÏÖÄÏóê Î∂ôÏó¨ÎÑ£Í≥† Î∞îÎ°ú Ïã§ÌñâÌïòÏÑ∏Ïöî (Shift + Enter)\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel, \n",
    "    ViTImageProcessor, \n",
    "    AutoTokenizer, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "# 1. Í≤ΩÎ°ú ÏÑ§Ï†ï (ÏïÑÍπå Î≥¥Ïó¨Ï£ºÏã† Ìè¥Îçî Íµ¨Ï°∞Ïóê ÎßûÏ∂§)\n",
    "# ---------------------------------------------------------\n",
    "JSON_FILE = \"dataset_v.02.json\"   # Î∞©Í∏à Îã§Ïö¥Î∞õÏùÄ Í∑∏ ÌååÏùº\n",
    "IMG_DIR = \"./data/train\"              # ÏïÑÍπå Î≥∏ 'images' Ìè¥Îçî\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# ÌòπÏãú Î™∞Îùº Í≤ΩÎ°ú Í∞ïÏ†ú Ïù¥Îèô (Î∞îÌÉïÌôîÎ©¥/second projectÎ°ú)\n",
    "try:\n",
    "    os.chdir(r\"c:\\Users\\ÌòÑÏÑùÏù¥\\Desktop\\second project\")\n",
    "    print(f\"üìÇ ÌòÑÏû¨ ÏûëÏóÖ Ìè¥Îçî: {os.getcwd()}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ÏÑ§Ï†ï ÌôïÏù∏\n",
    "if not os.path.exists(JSON_FILE):\n",
    "    print(f\"üö® '{JSON_FILE}' ÌååÏùºÏù¥ Ïïà Î≥¥ÏûÖÎãàÎã§! Ìè¥ÎçîÏóê Ïûò ÎÑ£ÏóàÎäîÏßÄ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.\")\n",
    "else:\n",
    "    print(f\"‚úÖ ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ ÌååÏùº ÌôïÏù∏Îê®: {JSON_FILE}\")\n",
    "\n",
    "if not os.path.exists(IMG_DIR):\n",
    "    print(f\"üö® '{IMG_DIR}' Ìè¥ÎçîÍ∞Ä Ïïà Î≥¥ÏûÖÎãàÎã§!\")\n",
    "else:\n",
    "    print(f\"‚úÖ Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî ÌôïÏù∏Îê®: {IMG_DIR}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
    "# ==========================================\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, json_file, root_dir, feature_extractor, tokenizer, max_length=64):\n",
    "        self.root_dir = root_dir\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = self._load_json(json_file)\n",
    "\n",
    "    def _load_json(self, json_file):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                raw_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            return []\n",
    "        \n",
    "        dataset = []\n",
    "        for item in raw_data:\n",
    "            try:\n",
    "                # Ï∫°ÏÖò Ï∂îÏ∂ú\n",
    "                caption = \"\"\n",
    "                if 'annotations' in item:\n",
    "                    caption = item['annotations'][0]['result'][0]['value']['text']\n",
    "                    if isinstance(caption, list): caption = \" \".join(caption)\n",
    "                elif 'text' in item:\n",
    "                    caption = item['text']\n",
    "                elif 'caption' in item:\n",
    "                    caption = item['caption']\n",
    "                \n",
    "                if not caption: continue\n",
    "\n",
    "                # Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú Ï∂îÏ∂ú (ÌååÏùºÎ™ÖÎßå Í∞ÄÏ†∏ÏôÄÏÑú images Ìè¥ÎçîÏôÄ Í≤∞Ìï©)\n",
    "                full_url = item['data']['image']\n",
    "                fname = full_url.split('files/')[-1] if 'files/' in full_url else os.path.basename(full_url)\n",
    "                img_path = os.path.join(self.root_dir, fname).replace(\"\\\\\", \"/\")\n",
    "                \n",
    "                if os.path.exists(img_path):\n",
    "                    dataset.append({\"image_path\": img_path, \"caption\": caption})\n",
    "            except:\n",
    "                continue\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        try:\n",
    "            image = Image.open(item['image_path']).convert(\"RGB\")\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0)) # ÏóêÎü¨ Î∞©ÏßÄÏö© Í≤ÄÏùÄÏÉâ Ïù¥ÎØ∏ÏßÄ\n",
    "\n",
    "        pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        tokenized = self.tokenizer(\n",
    "            item['caption'], \n",
    "            padding=\"max_length\", \n",
    "            max_length=self.max_length, \n",
    "            truncation=True\n",
    "        )\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": torch.tensor(tokenized.input_ids)}\n",
    "\n",
    "# ==========================================\n",
    "# 3. Î™®Îç∏ ÌïôÏäµ ÏãúÏûë\n",
    "# ==========================================\n",
    "# GTX 1070 Í∞êÏßÄ\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üî• ViT-GPT2 ÌïôÏäµ ÏãúÏûë (Device: {device})\")\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "MODEL_NAME = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "\n",
    "# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏÑ§Ï†ï\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.num_beams = 4\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "dataset = FlickrDataset(JSON_FILE, IMG_DIR, feature_extractor, tokenizer)\n",
    "\n",
    "if len(dataset) > 0:\n",
    "    print(f\"üìä Ï¥ù {len(dataset)}Ïû•Ïùò Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµÏùÑ Ï§ÄÎπÑÌï©ÎãàÎã§.\")\n",
    "    \n",
    "    # ÌïôÏäµ ÏÑ§Ï†ï\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./vit_gpt2_result\",\n",
    "        per_device_train_batch_size=4, # Î©îÎ™®Î¶¨ Î∂ÄÏ°±ÌïòÎ©¥ 2Î°ú Ï§ÑÏù¥ÏÑ∏Ïöî\n",
    "        num_train_epochs=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        fp16=False, # 1070 ÏïàÏ†ïÏÑ±\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "\n",
    "    # Ìä∏Î†àÏù¥ÎÑà Ïã§Ìñâ\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=feature_extractor,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"üöÄ ÌïôÏäµ ÏãúÏûë! (Ïû†Ïãú Í∏∞Îã§Î¶¨ÏÑ∏Ïöî...)\")\n",
    "    trainer.train()\n",
    "    print(\"üéâ ÌïôÏäµ ÏôÑÎ£å!\")\n",
    "else:\n",
    "    print(\"üö® ÌïôÏäµÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä 0Í∞úÏûÖÎãàÎã§. Ïù¥ÎØ∏ÏßÄ ÌååÏùºÎ™ÖÍ≥º JSON Ï†ïÎ≥¥Î•º Îã§Ïãú ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f295364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÌïôÏäµÎêú ÏµúÏã† Î™®Îç∏ Î°úÎìú: ./vit_gpt2_result\\checkpoint-2710\n",
      "üî• [System] Ï∫°ÏÖò ÏÉùÏÑ±Í∏∞ Í∞ÄÎèô (English Only) | Device: cuda\n",
      "‚è≥ Î™®Îç∏ Î°úÎî© Ï§ë...\n",
      "\n",
      "‚ú® === [AI ÏòÅÏñ¥ Ï∫°ÏÖò Í≤∞Í≥º] ===\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 515.jpg\n",
      "ü§ñ AI: \" all lizard a bottle a bottle a bottle a bottle\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 423.jpg\n",
      "ü§ñ AI: \" greyrot perched a on table\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 027.jpg\n",
      "ü§ñ AI: \"rocile in f area\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 372.jpg\n",
      "ü§ñ AI: \" clock a dial black white dial blackals a\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 594.jpg\n",
      "ü§ñ AI: \" blue black be with backpack on ground\"\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# 1. ÌïôÏäµÎêú Î™®Îç∏(ViT-GPT2) ÏûêÎèô ÌÉêÏÉâ\n",
    "# ==========================================\n",
    "ckpt_path = \"./vit_gpt2_result\"\n",
    "model_to_use = \"\"\n",
    "\n",
    "# Í∞ÄÏû• ÏµúÍ∑ºÏóê Ï†ÄÏû•Îêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï∞æÍ∏∞\n",
    "if os.path.exists(ckpt_path):\n",
    "    ckpts = glob.glob(os.path.join(ckpt_path, \"checkpoint-*\"))\n",
    "    if len(ckpts) > 0:\n",
    "        model_to_use = max(ckpts, key=os.path.getctime)\n",
    "        print(f\"‚úÖ ÌïôÏäµÎêú ÏµúÏã† Î™®Îç∏ Î°úÎìú: {model_to_use}\")\n",
    "    else:\n",
    "        model_to_use = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "else:\n",
    "    model_to_use = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "IMG_DIR = \"./data/train\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üî• [System] Ï∫°ÏÖò ÏÉùÏÑ±Í∏∞ Í∞ÄÎèô (English Only) | Device: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Î™®Îç∏ Î°úÎìú (KeyError ÏôÑÎ≤Ω Î∞©ÏßÄ)\n",
    "# ==========================================\n",
    "BASE_MODEL = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "print(\"‚è≥ Î™®Îç∏ Î°úÎî© Ï§ë...\")\n",
    "try:\n",
    "    # üß† Îáå (Model): ÎÇ¥Í∞Ä ÌïôÏäµÏãúÌÇ® Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(model_to_use).to(device)\n",
    "    \n",
    "    # üìñ Îã®Ïñ¥Ïû• (Tokenizer): ÏõêÎ≥∏ ÏÑ§Ï†ï ÏÇ¨Ïö© (ÏóêÎü¨ Î∞©ÏßÄÏö©)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "    feature_extractor = ViTImageProcessor.from_pretrained(BASE_MODEL)\n",
    "except Exception as e:\n",
    "    print(f\"üö® Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==========================================\n",
    "# 3. Ï∫°ÏÖò ÏÉùÏÑ± Ìï®Ïàò\n",
    "# ==========================================\n",
    "def generate_caption(image_path):\n",
    "    try:\n",
    "        i_image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = feature_extractor(images=[i_image], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        \n",
    "        # Îπî ÏÑúÏπò (Beam Search): Í∞ÄÏû• ÌôïÎ•† ÎÜíÏùÄ Î¨∏Ïû• 4Í∞úÎ•º ÎπÑÍµêÌï¥ÏÑú ÏµúÏ†ÅÏùò Î¨∏Ïû• ÏÑ†ÌÉù\n",
    "        output_ids = model.generate(\n",
    "            pixel_values, \n",
    "            max_length=50, \n",
    "            num_beams=4, \n",
    "            early_stopping=True\n",
    "        )\n",
    "        caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# ==========================================\n",
    "# 4. Í≤∞Í≥º ÌôïÏù∏ (ÎûúÎç§ 5Ïû•)\n",
    "# ==========================================\n",
    "print(\"\\n‚ú® === [AI ÏòÅÏñ¥ Ï∫°ÏÖò Í≤∞Í≥º] ===\")\n",
    "all_imgs = [os.path.join(dp, f) for dp, dn, fn in os.walk(IMG_DIR) for f in fn if f.endswith(('.jpg','.png', '.jpeg'))]\n",
    "\n",
    "if all_imgs:\n",
    "    # ÎûúÎç§ 5Ïû• ÎΩëÍ∏∞\n",
    "    test_imgs = random.sample(all_imgs, min(5, len(all_imgs)))\n",
    "    \n",
    "    for img_path in test_imgs:\n",
    "        caption = generate_caption(img_path)\n",
    "        \n",
    "        print(f\"\\nüñºÔ∏è ÌååÏùº: {os.path.basename(img_path)}\")\n",
    "        print(f\"ü§ñ AI: \\\"{caption}\\\"\")\n",
    "        print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"üö® Ïù¥ÎØ∏ÏßÄ Ìè¥ÎçîÍ∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b746191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• [System] Ï∫°ÏÖò ÍµêÏ†ïÍ∏∞ Í∞ÄÎèô | Model: ./vit_gpt2_result\\checkpoint-2710\n",
      "\n",
      "‚ú® === [ÍµêÏ†ïÎêú ÏòÅÏñ¥ Ï∫°ÏÖò] ===\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 512.jpg\n",
      "ü§ñ AI: \" assaultr being in a stance a_ifle held by human\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 371.jpg\n",
      "ü§ñ AI: \" all lizard rough pl scales in person hand\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 518.jpg\n",
      "ü§ñ AI: \" bright americ chleon swimming the\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 417.jpg\n",
      "ü§ñ AI: \" clock a dial black white\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 074.jpg\n",
      "ü§ñ AI: \" brownel on table\"\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üöÄ Ï∫°ÏÖò ÏÉùÏÑ±Í∏∞ (Îßê ÎçîÎì¨Í∏∞ ÍµêÏ†ï Î≤ÑÏ†Ñ)\n",
    "# ==========================================\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import random\n",
    "\n",
    "# 1. Î™®Îç∏ ÏÑ§Ï†ï\n",
    "ckpt_path = \"./vit_gpt2_result\"\n",
    "model_to_use = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    ckpts = glob.glob(os.path.join(ckpt_path, \"checkpoint-*\"))\n",
    "    if len(ckpts) > 0:\n",
    "        model_to_use = max(ckpts, key=os.path.getctime)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üî• [System] Ï∫°ÏÖò ÍµêÏ†ïÍ∏∞ Í∞ÄÎèô | Model: {model_to_use}\")\n",
    "\n",
    "# 2. Î°úÎìú\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_to_use).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# 3. ÏÉùÏÑ± Ìï®Ïàò (Ïó¨Í∏∞Î•º Í≥†Ï≥§ÏäµÎãàÎã§!)\n",
    "def generate_caption(image_path):\n",
    "    try:\n",
    "        i_image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = feature_extractor(images=[i_image], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        \n",
    "        # Îπî ÏÑúÏπò + Î∞òÎ≥µ Î∞©ÏßÄ ÏòµÏÖò Ï∂îÍ∞Ä\n",
    "        output_ids = model.generate(\n",
    "            pixel_values, \n",
    "            max_length=50, \n",
    "            num_beams=4, \n",
    "            early_stopping=True,\n",
    "            \n",
    "            # üëá ÌïµÏã¨: Îßê ÎçîÎì¨Í∏∞ Î∞©ÏßÄ ÏòµÏÖò\n",
    "            no_repeat_ngram_size=2,  # Í∞ôÏùÄ Îã®Ïñ¥ 2Î≤à Î∞òÎ≥µ Í∏àÏßÄ\n",
    "            repetition_penalty=2.0,  # Î∞òÎ≥µÌïòÎ©¥ Î≤åÏ†ê Î∂ÄÏó¨\n",
    "        )\n",
    "        caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# 4. Í≤∞Í≥º ÌôïÏù∏\n",
    "print(\"\\n‚ú® === [ÍµêÏ†ïÎêú ÏòÅÏñ¥ Ï∫°ÏÖò] ===\")\n",
    "IMG_DIR = \"./data/train\"\n",
    "all_imgs = [os.path.join(dp, f) for dp, dn, fn in os.walk(IMG_DIR) for f in fn if f.endswith(('.jpg','.png'))]\n",
    "\n",
    "if all_imgs:\n",
    "    test_imgs = random.sample(all_imgs, min(5, len(all_imgs)))\n",
    "    for img_path in test_imgs:\n",
    "        caption = generate_caption(img_path)\n",
    "        print(f\"\\nüñºÔ∏è ÌååÏùº: {os.path.basename(img_path)}\")\n",
    "        print(f\"ü§ñ AI: \\\"{caption}\\\"\")\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b6cac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• [Final System] ÌïúÏòÅ ÌÜµÌï© AI Í∞ÄÎèô | Model: ./vit_gpt2_result\\checkpoint-2710\n",
      "\n",
      "‚ú® === [üá∞üá∑ ÌïúÍµ≠Ïñ¥ Ï∫°ÏÖò ÏµúÏ¢Ö Í≤∞Í≥º] ===\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 600.jpg\n",
      "üá∫üá∏ ÏòÅÏñ¥:  interior of ambulance aile in room\n",
      "üá∞üá∑ ÌïúÍµ≠Ïñ¥: Î∞©Ïóê ÏûàÎäî Íµ¨Í∏âÏ∞® ÎÇ¥Î∂Ä\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 663.jpg\n",
      "üá∫üá∏ ÏòÅÏñ¥:  whole redan served aan with and claws on table\n",
      "üá∞üá∑ ÌïúÍµ≠Ïñ¥: Redan Ï†ÑÏ≤¥Í∞Ä ÌÖåÏù¥Î∏îÏóê Î∞úÌÜ±Í≥º Î∞úÌÜ±ÏùÑ ÏñπÏñ¥ Ï†úÍ≥µÌñàÏäµÎãàÎã§.\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 224.jpg\n",
      "üá∫üá∏ ÏòÅÏñ¥:  artoke a stem a anich plant\n",
      "üá∞üá∑ ÌïúÍµ≠Ïñ¥: artoke Ï§ÑÍ∏∞ anich ÏãùÎ¨º\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 262.jpg\n",
      "üá∫üá∏ ÏòÅÏñ¥:  red whitear with spots the on grass\n",
      "üá∞üá∑ ÌïúÍµ≠Ïñ¥: ÏûîÎîîÏóê Î∞òÏ†êÏù¥ ÏûàÎäî Îπ®Í∞ÑÏÉâ Ìù∞ÏÉâ\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 460.jpg\n",
      "üá∫üá∏ ÏòÅÏñ¥:  artoke on green anich hanging aich flower\n",
      "üá∞üá∑ ÌïúÍµ≠Ïñ¥: ÎÖπÏÉâ anichÏóê artoke Îß§Îã¨Î†§ aich ÍΩÉ\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Íµ¨Í∏Ä Î≤àÏó≠ ÎùºÏù¥Î∏åÎü¨Î¶¨ (ÏóÜÏúºÎ©¥ ÏÑ§Ïπò)\n",
    "try:\n",
    "    from deep_translator import GoogleTranslator\n",
    "except ImportError:\n",
    "    !pip install deep-translator\n",
    "    from deep_translator import GoogleTranslator\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# 1. Î™®Îç∏ Î°úÎìú (ÏµúÏã† Î≤ÑÏ†Ñ ÏûêÎèô ÌÉêÏÉâ)\n",
    "# ==========================================\n",
    "ckpt_path = \"./vit_gpt2_result\"\n",
    "model_to_use = \"nlpconnect/vit-gpt2-image-captioning\" # Í∏∞Î≥∏Í∞í\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    ckpts = glob.glob(os.path.join(ckpt_path, \"checkpoint-*\"))\n",
    "    if len(ckpts) > 0:\n",
    "        model_to_use = max(ckpts, key=os.path.getctime)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üî• [Final System] ÌïúÏòÅ ÌÜµÌï© AI Í∞ÄÎèô | Model: {model_to_use}\")\n",
    "\n",
    "# 2. ÏóîÏßÑ Î°úÎìú\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_to_use).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Ï∫°ÏÖò ÏÉùÏÑ± + Î≤àÏó≠ Ìï®Ïàò (ÏôÑÏ†ÑÏ≤¥)\n",
    "# ==========================================\n",
    "def generate_and_translate(image_path):\n",
    "    try:\n",
    "        # (1) Ïù¥ÎØ∏ÏßÄ Ïù∏Ïãù\n",
    "        i_image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = feature_extractor(images=[i_image], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        \n",
    "        # (2) ÏòÅÏñ¥ Ï∫°ÏÖò ÏÉùÏÑ± (ÎßêÎçîÎì¨Í∏∞ ÍµêÏ†ï Ï†ÅÏö©)\n",
    "        output_ids = model.generate(\n",
    "            pixel_values, \n",
    "            max_length=50, \n",
    "            num_beams=4, \n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,  # Î∞òÎ≥µ Í∏àÏßÄ\n",
    "            repetition_penalty=2.0   # ÌéòÎÑêÌã∞\n",
    "        )\n",
    "        eng_caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # (3) ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ (Íµ¨Í∏Ä ÏóîÏßÑ)\n",
    "        kor_caption = GoogleTranslator(source='auto', target='ko').translate(eng_caption)\n",
    "        \n",
    "        return eng_caption, kor_caption\n",
    "    except Exception as e:\n",
    "        return \"Error\", \"ÏóêÎü¨ Î∞úÏÉù\"\n",
    "\n",
    "# ==========================================\n",
    "# 4. ÏµúÏ¢Ö ÏãúÏó∞ (Demo)\n",
    "# ==========================================\n",
    "print(\"\\n‚ú® === [üá∞üá∑ ÌïúÍµ≠Ïñ¥ Ï∫°ÏÖò ÏµúÏ¢Ö Í≤∞Í≥º] ===\")\n",
    "IMG_DIR = \"./data/train\"\n",
    "all_imgs = [os.path.join(dp, f) for dp, dn, fn in os.walk(IMG_DIR) for f in fn if f.endswith(('.jpg','.png'))]\n",
    "\n",
    "if all_imgs:\n",
    "    # ÎûúÎç§ 5Ïû• ÌÖåÏä§Ìä∏\n",
    "    test_imgs = random.sample(all_imgs, min(5, len(all_imgs)))\n",
    "    \n",
    "    for img_path in test_imgs:\n",
    "        eng, kor = generate_and_translate(img_path)\n",
    "        \n",
    "        print(f\"\\nüñºÔ∏è ÌååÏùº: {os.path.basename(img_path)}\")\n",
    "        print(f\"üá∫üá∏ ÏòÅÏñ¥: {eng}\")\n",
    "        print(f\"üá∞üá∑ ÌïúÍµ≠Ïñ¥: {kor}\")\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f953a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-2.14.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ÌòÑÏÑùÏù¥\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.12.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ÌòÑÏÑùÏù¥\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.12.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ÌòÑÏÑùÏù¥\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\ÌòÑÏÑùÏù¥\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ÌòÑÏÑùÏù¥\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\ÌòÑÏÑùÏù¥\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ÌòÑÏÑùÏù¥\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ÌòÑÏÑùÏù¥\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ÌòÑÏÑùÏù¥\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-2.14.0-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.12.0-cp312-cp312-win_amd64.whl (205 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 11.2 MB/s eta 0:00:00\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, sniffio, pydantic-core, jiter, distro, annotated-types, pydantic, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 jiter-0.12.0 openai-2.14.0 pydantic-2.12.5 pydantic-core-2.41.5 sniffio-1.3.1 typing-inspection-0.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• [Hybrid System] ViT-GPT2 + ChatGPT Í∞ÄÎèô | Model: ./vit_gpt2_result\\checkpoint-2710\n",
      "\n",
      "‚ú® === [ChatGPTÍ∞Ä Îã§Îì¨ÏùÄ ÏµúÏ¢Ö Í≤∞Í≥º] ===\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 212.jpg\n",
      "üá∫üá∏ Î™®Îç∏Ïù¥ Î≥∏ Í≤É:  black with backpack on ground\n",
      "üá∞üá∑ GPTÏùò Î≤àÏó≠: GPT Ìò∏Ï∂ú Ïã§Ìå®: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 621.jpg\n",
      "üá∫üá∏ Î™®Îç∏Ïù¥ Î≥∏ Í≤É:  bright americ chleon standing the\n",
      "üá∞üá∑ GPTÏùò Î≤àÏó≠: GPT Ìò∏Ï∂ú Ïã§Ìå®: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 088.jpg\n",
      "üá∫üá∏ Î™®Îç∏Ïù¥ Î≥∏ Í≤É: , and stripes chleon on leaf\n",
      "üá∞üá∑ GPTÏùò Î≤àÏó≠: GPT Ìò∏Ï∂ú Ïã§Ìå®: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n",
    "try:\n",
    "    import openai\n",
    "except ImportError:\n",
    "    !pip install openai\n",
    "    import openai\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# üõë [ÏÑ§Ï†ï] Ïó¨Í∏∞Ïóê OpenAI API ÌÇ§Î•º ÎÑ£ÏúºÏÑ∏Ïöî!\n",
    "# ==========================================\n",
    "OPENAI_API_KEY = \"sk-...\"  # Ïó¨Í∏∞Ïóê Ïã§Ï†ú ÌÇ§Î•º ÏûÖÎ†•Ìï¥Ïïº Ìï©ÎãàÎã§. (sk-Î°ú ÏãúÏûë)\n",
    "\n",
    "# ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÑ§Ï†ï (ÏµúÏã† Î≤ÑÏ†Ñ Í∏∞Ï§Ä)\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# ==========================================\n",
    "# 1. ÎÇ¥ Î™®Îç∏(ViT-GPT2) Î°úÎìú\n",
    "# ==========================================\n",
    "ckpt_path = \"./vit_gpt2_result\"\n",
    "model_to_use = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    ckpts = glob.glob(os.path.join(ckpt_path, \"checkpoint-*\"))\n",
    "    if len(ckpts) > 0:\n",
    "        model_to_use = max(ckpts, key=os.path.getctime)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üî• [Hybrid System] ViT-GPT2 + ChatGPT Í∞ÄÎèô | Model: {model_to_use}\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_to_use).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Í∏∞Îä• Ìï®Ïàò (GPTÏóêÍ≤å Î≤àÏó≠ ÏãúÌÇ§Í∏∞)\n",
    "# ==========================================\n",
    "def generate_caption(image_path):\n",
    "    try:\n",
    "        i_image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = feature_extractor(images=[i_image], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        \n",
    "        # ÏòÅÏñ¥ Ï∫°ÏÖò ÏÉùÏÑ±\n",
    "        output_ids = model.generate(\n",
    "            pixel_values, \n",
    "            max_length=50, \n",
    "            num_beams=4, \n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            repetition_penalty=2.0\n",
    "        )\n",
    "        eng_caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return eng_caption\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def ask_gpt_translate(text):\n",
    "    if not text: return \"ÎÇ¥Ïö© ÏóÜÏùå\"\n",
    "    try:\n",
    "        # GPTÏóêÍ≤å Ïó≠Ìï†ÏùÑ Î∂ÄÏó¨Ìï¥ÏÑú Î≤àÏó≠ ÏöîÏ≤≠\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\", # ÎòêÎäî gpt-4o (ÎπÑÏö© Ï∞®Ïù¥ ÏûàÏùå)\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"ÎÑàÎäî Ï†ÑÎ¨∏ Î≤àÏó≠Í∞ÄÏïº. ÏûÖÎ†•Îêú ÏòÅÏñ¥ Ïù¥ÎØ∏ÏßÄ Ï∫°ÏÖòÏùÑ Í∞ÄÏû• ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌïúÍµ≠Ïñ¥Î°ú ÏùòÏó≠Ìï¥Ï§ò. ÏÑ§Î™ÖÏ°∞Î°ú ÎÅùÎÇ¥Ï§ò (Ïòà: ~Í∞Ä ÏûàÏäµÎãàÎã§).\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Translate this caption: '{text}'\"}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"GPT Ìò∏Ï∂ú Ïã§Ìå®: {e}\"\n",
    "\n",
    "# ==========================================\n",
    "# 3. ÏµúÏ¢Ö Í≤∞Í≥º ÌôïÏù∏\n",
    "# ==========================================\n",
    "print(\"\\n‚ú® === [ChatGPTÍ∞Ä Îã§Îì¨ÏùÄ ÏµúÏ¢Ö Í≤∞Í≥º] ===\")\n",
    "IMG_DIR = \"./data/train\"\n",
    "all_imgs = [os.path.join(dp, f) for dp, dn, fn in os.walk(IMG_DIR) for f in fn if f.endswith(('.jpg','.png'))]\n",
    "\n",
    "if all_imgs:\n",
    "    test_imgs = random.sample(all_imgs, min(3, len(all_imgs)))\n",
    "    \n",
    "    for img_path in test_imgs:\n",
    "        eng_cap = generate_caption(img_path)\n",
    "        if eng_cap:\n",
    "            # Ïó¨Í∏∞ÏÑú GPTÍ∞Ä Í∞úÏûÖ!\n",
    "            kor_cap = ask_gpt_translate(eng_cap)\n",
    "            \n",
    "            print(f\"\\nüñºÔ∏è ÌååÏùº: {os.path.basename(img_path)}\")\n",
    "            print(f\"üá∫üá∏ Î™®Îç∏Ïù¥ Î≥∏ Í≤É: {eng_cap}\")\n",
    "            print(f\"üá∞üá∑ GPTÏùò Î≤àÏó≠: {kor_cap}\")\n",
    "            print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed2fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• [System] ÏµúÏ¢Ö ÌååÏù¥ÌîÑÎùºÏù∏ Í∞ÄÎèô | Model: ./vit_gpt2_result\\checkpoint-2710\n",
      "\n",
      "‚ú® === [üá∞üá∑ AI ÏµúÏ¢Ö Î∂ÑÏÑù Í≤∞Í≥º] ===\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 540.jpg\n",
      "üá∫üá∏ Vision AI: \" tray bag featuring brown surface placed a a bag withesame on table\"\n",
      "üá∞üá∑ Translation: \"Í∞àÏÉâ ÌëúÎ©¥Ïù¥ ÌäπÏßïÏù∏ Ìä∏Î†àÏù¥ Í∞ÄÎ∞© ÌÖåÏù¥Î∏î ÏúÑÏóê ÎÜìÏù∏ Í∞ÄÎ∞©\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 087.jpg\n",
      "üá∫üá∏ Vision AI: \" largeliner on t taking from airport\"\n",
      "üá∞üá∑ Translation: \"Í≥µÌï≠ÏóêÏÑú Ïù¥ÎèôÌïòÎäî ÎåÄÌòï Ïó¨Í∞ùÍ∏∞\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 208.jpg\n",
      "üá∫üá∏ Vision AI: \" largeliner on platform a platform\"\n",
      "üá∞üá∑ Translation: \"ÌîåÎû´ÌèºÏùò ÎåÄÌòï ÎùºÏù¥ÎÑà ÌîåÎû´Ìèº\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 337.jpg\n",
      "üá∫üá∏ Vision AI: \" assaultr being in person hand\"\n",
      "üá∞üá∑ Translation: \"Í∞ÄÌï¥ÏûêÍ∞Ä ÏßÅÏ†ë ÏÜêÏóê\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 364.jpg\n",
      "üá∫üá∏ Vision AI: \" brownired Ter laying a lawn a cage a Aalerier a yard\"\n",
      "üá∞üá∑ Translation: \"Í∞àÏÉâÎπõÏùò TerÎäî ÏûîÎîîÎ∞≠Ïóê ÏÉàÏû•ÏùÑ ÍπîÍ≥† AalerierÎ•º ÎßàÎãπÏóê ÍπîÏïòÏäµÎãàÎã§.\"\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò (Íµ¨Í∏Ä Î≤àÏó≠Í∏∞)\n",
    "try:\n",
    "    from deep_translator import GoogleTranslator\n",
    "except ImportError:\n",
    "    !pip install deep-translator\n",
    "    from deep_translator import GoogleTranslator\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# 1. Í∞ÄÏû• ÎòëÎòëÌïú Î™®Îç∏(Epoch 10) ÏûêÎèô Î°úÎìú\n",
    "# ==========================================\n",
    "ckpt_path = \"./vit_gpt2_result\"\n",
    "model_to_use = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "# Í∞ÄÏû• ÏµúÍ∑º Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ï∞æÍ∏∞\n",
    "if os.path.exists(ckpt_path):\n",
    "    ckpts = glob.glob(os.path.join(ckpt_path, \"checkpoint-*\"))\n",
    "    if len(ckpts) > 0:\n",
    "        model_to_use = max(ckpts, key=os.path.getctime)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üî• [System] ÏµúÏ¢Ö ÌååÏù¥ÌîÑÎùºÏù∏ Í∞ÄÎèô | Model: {model_to_use}\")\n",
    "\n",
    "# 2. ÏóîÏßÑ Î°úÎî©\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_to_use).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Ï∫°ÏÖò ÏÉùÏÑ± + Î≤àÏó≠ (ÏôÑÏÑ±Ìòï)\n",
    "# ==========================================\n",
    "def generate_final_caption(image_path):\n",
    "    try:\n",
    "        # (1) Ïù¥ÎØ∏ÏßÄ Ïù∏Ïãù\n",
    "        i_image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = feature_extractor(images=[i_image], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        \n",
    "        # (2) ÏòÅÏñ¥ Ï∫°ÏÖò (ÎßêÎçîÎì¨Í∏∞ Î∞©ÏßÄ ÏòµÏÖò Ï†ÅÏö©)\n",
    "        output_ids = model.generate(\n",
    "            pixel_values, \n",
    "            max_length=50, \n",
    "            num_beams=4, \n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,  # Í∞ôÏùÄ Îã®Ïñ¥ Î∞òÎ≥µ Í∏àÏßÄ\n",
    "            repetition_penalty=2.0   # ÌéòÎÑêÌã∞ Î∂ÄÏó¨\n",
    "        )\n",
    "        eng_caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # (3) ÌïúÍµ≠Ïñ¥ Î≤àÏó≠ (Íµ¨Í∏Ä ÏóîÏßÑ ÏÇ¨Ïö©)\n",
    "        kor_caption = GoogleTranslator(source='auto', target='ko').translate(eng_caption)\n",
    "        \n",
    "        return eng_caption, kor_caption\n",
    "    except Exception as e:\n",
    "        return \"Error\", \"Î∂ÑÏÑù Ïã§Ìå®\"\n",
    "\n",
    "# ==========================================\n",
    "# 3. ÏµúÏ¢Ö Í≤∞Í≥º Î¶¨Ìè¨Ìä∏\n",
    "# ==========================================\n",
    "print(\"\\n‚ú® === [üá∞üá∑ AI ÏµúÏ¢Ö Î∂ÑÏÑù Í≤∞Í≥º] ===\")\n",
    "IMG_DIR = \"./data/train\"\n",
    "all_imgs = [os.path.join(dp, f) for dp, dn, fn in os.walk(IMG_DIR) for f in fn if f.endswith(('.jpg','.png'))]\n",
    "\n",
    "if all_imgs:\n",
    "    # ÎûúÎç§ 5Ïû• ÌÖåÏä§Ìä∏\n",
    "    test_imgs = random.sample(all_imgs, min(5, len(all_imgs)))\n",
    "    \n",
    "    for img_path in test_imgs:\n",
    "        eng, kor = generate_final_caption(img_path)\n",
    "        \n",
    "        print(f\"\\nüñºÔ∏è ÌååÏùº: {os.path.basename(img_path)}\")\n",
    "        print(f\"üá∫üá∏ Vision AI: \\\"{eng}\\\"\")\n",
    "        print(f\"üá∞üá∑ Translation: \\\"{kor}\\\"\")\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596e117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "C:\\Users\\ÌòÑÏÑùÏù¥\\AppData\\Local\\Temp\\ipykernel_25168\\2014057114.py:6: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• [Gemini Hybrid] ViT-GPT2 + Google Gemini Í∞ÄÎèô | Model: ./vit_gpt2_result\\checkpoint-2710\n",
      "\n",
      "‚ú® === [GeminiÍ∞Ä Îã§Îì¨ÏùÄ ÏµúÏ¢Ö Í≤∞Í≥º] ===\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 435.jpg\n",
      "üá∫üá∏ Vision AI: \" all lizard rough pl scales on ground\"\n",
      "üíé Gemini KO: \"Gemini Ìò∏Ï∂ú Ïã§Ìå®: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 223.jpg\n",
      "üá∫üá∏ Vision AI: \" and stripes chleon on branch\"\n",
      "üíé Gemini KO: \"Gemini Ìò∏Ï∂ú Ïã§Ìå®: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 547.jpg\n",
      "üá∫üá∏ Vision AI: \" arianc sitting ac riding a anab_amel\"\n",
      "üíé Gemini KO: \"Gemini Ìò∏Ï∂ú Ïã§Ìå®: 404 models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\"\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Ï†úÎØ∏ÎÇòÏù¥ ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "except ImportError:\n",
    "    !pip install -q -U google-generativeai\n",
    "    import google.generativeai as genai\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# üõë [ÏÑ§Ï†ï] Ïó¨Í∏∞Ïóê Gemini API ÌÇ§Î•º Î∂ôÏó¨ÎÑ£ÏúºÏÑ∏Ïöî!\n",
    "# ==========================================\n",
    "GOOGLE_API_KEY = \"AIzaSyAw7njqN8c3jizsXfEq8NGYixvjQmReuEM\"  # üëà Ïó¨Í∏∞Ïóê Î≥µÏÇ¨Ìïú ÌÇ§ ÏûÖÎ†• (Îî∞Ïò¥Ìëú Ïú†ÏßÄ)\n",
    "\n",
    "# Ï†úÎØ∏ÎÇòÏù¥ ÏÑ§Ï†ï\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "# Í∞ÄÏû• Îπ†Î•¥Í≥† ÎòëÎòëÌïú ÏµúÏã† Î™®Îç∏ ÏÑ†ÌÉù (Gemini 1.5 Flash Í∂åÏû•)\n",
    "gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "# ==========================================\n",
    "# 1. ÎÇ¥ Î™®Îç∏(ViT-GPT2) Î°úÎìú\n",
    "# ==========================================\n",
    "ckpt_path = \"./vit_gpt2_result\"\n",
    "model_to_use = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    ckpts = glob.glob(os.path.join(ckpt_path, \"checkpoint-*\"))\n",
    "    if len(ckpts) > 0:\n",
    "        model_to_use = max(ckpts, key=os.path.getctime)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üî• [Gemini Hybrid] ViT-GPT2 + Google Gemini Í∞ÄÎèô | Model: {model_to_use}\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_to_use).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Í∏∞Îä• Ìï®Ïàò (GeminiÏóêÍ≤å Î≤àÏó≠ Î∞è ÍµêÏ†ï ÏöîÏ≤≠)\n",
    "# ==========================================\n",
    "def generate_vision_caption(image_path):\n",
    "    try:\n",
    "        i_image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = feature_extractor(images=[i_image], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        \n",
    "        output_ids = model.generate(\n",
    "            pixel_values, \n",
    "            max_length=50, \n",
    "            num_beams=4, \n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            repetition_penalty=2.0\n",
    "        )\n",
    "        eng_caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return eng_caption\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def ask_gemini_translate(text):\n",
    "    if not text: return \"ÎÇ¥Ïö© ÏóÜÏùå\"\n",
    "    try:\n",
    "        # GeminiÏóêÍ≤å 'Ïò§ÌÉÄ ÏàòÏ†ï'Í≥º 'ÏûêÏó∞Ïä§Îü¨Ïö¥ Î≤àÏó≠'ÏùÑ ÎèôÏãúÏóê ÏöîÏ≤≠\n",
    "        prompt = f\"\"\"\n",
    "        You are a professional translator. \n",
    "        The following text is an image caption generated by an AI, so it may contain typos or abbreviations (e.g., 'assaultr' -> 'assault rifle').\n",
    "        \n",
    "        1. Correct the typos and understand the context.\n",
    "        2. Translate it into natural, descriptive Korean.\n",
    "        3. Just output the Korean sentence.\n",
    "\n",
    "        Input text: \"{text}\"\n",
    "        \"\"\"\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Gemini Ìò∏Ï∂ú Ïã§Ìå®: {e}\"\n",
    "\n",
    "# ==========================================\n",
    "# 3. ÏµúÏ¢Ö Í≤∞Í≥º ÌôïÏù∏\n",
    "# ==========================================\n",
    "print(\"\\n‚ú® === [GeminiÍ∞Ä Îã§Îì¨ÏùÄ ÏµúÏ¢Ö Í≤∞Í≥º] ===\")\n",
    "IMG_DIR = \"./data/train\"\n",
    "all_imgs = [os.path.join(dp, f) for dp, dn, fn in os.walk(IMG_DIR) for f in fn if f.endswith(('.jpg','.png'))]\n",
    "\n",
    "if all_imgs:\n",
    "    # ÎûúÎç§ 3Ïû• ÌÖåÏä§Ìä∏\n",
    "    test_imgs = random.sample(all_imgs, min(3, len(all_imgs)))\n",
    "    \n",
    "    for img_path in test_imgs:\n",
    "        eng_cap = generate_vision_caption(img_path)\n",
    "        if eng_cap:\n",
    "            # GeminiÍ∞Ä ÍµêÏ†ï Î∞è Î≤àÏó≠\n",
    "            kor_cap = ask_gemini_translate(eng_cap)\n",
    "            \n",
    "            print(f\"\\nüñºÔ∏è ÌååÏùº: {os.path.basename(img_path)}\")\n",
    "            print(f\"üá∫üá∏ Vision AI: \\\"{eng_cap}\\\"\")\n",
    "            print(f\"üíé Gemini KO: \\\"{kor_cap}\\\"\")\n",
    "            print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"üö® Ïù¥ÎØ∏ÏßÄ Ìè¥ÎçîÍ∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a91cecf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• [Gemini Hybrid] ViT-GPT2 + Google Gemini(Pro) Í∞ÄÎèô | Model: ./vit_gpt2_result\\checkpoint-2710\n",
      "\n",
      "‚ú® === [Gemini(Pro)Í∞Ä Îã§Îì¨ÏùÄ ÏµúÏ¢Ö Í≤∞Í≥º] ===\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 291.jpg\n",
      "üá∫üá∏ Vision AI: \" brown bear standing they in grass\"\n",
      "üíé Gemini KO: \"Gemini Ìò∏Ï∂ú Ïã§Ìå®: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 226.jpg\n",
      "üá∫üá∏ Vision AI: \" largeliner on of airportarmac\"\n",
      "üíé Gemini KO: \"Gemini Ìò∏Ï∂ú Ïã§Ìå®: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\"\n",
      "------------------------------\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 437.jpg\n",
      "üá∫üá∏ Vision AI: \" balde featuring spread, against blue sky\"\n",
      "üíé Gemini KO: \"Gemini Ìò∏Ï∂ú Ïã§Ìå®: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\"\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Ï†úÎØ∏ÎÇòÏù¥ ÎùºÏù¥Î∏åÎü¨Î¶¨ (Í∏∞Ï°¥ Í∑∏ÎåÄÎ°ú)\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "except ImportError:\n",
    "    !pip install -q -U google-generativeai\n",
    "    import google.generativeai as genai\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# üõë [ÏÑ§Ï†ï] Î≥µÏÇ¨Ìï¥Îëî API ÌÇ§Î•º Í∑∏ÎåÄÎ°ú Ïì∞ÏÑ∏Ïöî!\n",
    "# ==========================================\n",
    "GOOGLE_API_KEY = \"AIzaSyAw7njqN8c3jizsXfEq8NGYixvjQmReuEM\"  # üëà ÏïÑÍπå Î∞úÍ∏âÎ∞õÏùÄ ÌÇ§ Í∑∏ÎåÄÎ°ú (Îî∞Ïò¥Ìëú Ïú†ÏßÄ)\n",
    "\n",
    "# Ï†úÎØ∏ÎÇòÏù¥ ÏÑ§Ï†ï\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# üëá [ÏàòÏ†ïÎê®] Î™®Îç∏ Ïù¥Î¶ÑÏùÑ 'gemini-1.5-flash' -> 'gemini-pro'Î°ú Î≥ÄÍ≤Ω (Ìò∏ÌôòÏÑ± Ìï¥Í≤∞)\n",
    "gemini_model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "# ==========================================\n",
    "# 1. ÎÇ¥ Î™®Îç∏(ViT-GPT2) Î°úÎìú\n",
    "# ==========================================\n",
    "ckpt_path = \"./vit_gpt2_result\"\n",
    "model_to_use = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    ckpts = glob.glob(os.path.join(ckpt_path, \"checkpoint-*\"))\n",
    "    if len(ckpts) > 0:\n",
    "        model_to_use = max(ckpts, key=os.path.getctime)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üî• [Gemini Hybrid] ViT-GPT2 + Google Gemini(Pro) Í∞ÄÎèô | Model: {model_to_use}\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_to_use).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Í∏∞Îä• Ìï®Ïàò\n",
    "# ==========================================\n",
    "def generate_vision_caption(image_path):\n",
    "    try:\n",
    "        i_image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = feature_extractor(images=[i_image], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        \n",
    "        output_ids = model.generate(\n",
    "            pixel_values, \n",
    "            max_length=50, \n",
    "            num_beams=4, \n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            repetition_penalty=2.0\n",
    "        )\n",
    "        eng_caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return eng_caption\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def ask_gemini_translate(text):\n",
    "    if not text: return \"ÎÇ¥Ïö© ÏóÜÏùå\"\n",
    "    try:\n",
    "        # Prompt: Ïò§ÌÉÄ ÏàòÏ†ï + ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌïúÍµ≠Ïñ¥ ÏùòÏó≠ ÏöîÏ≤≠\n",
    "        prompt = f\"\"\"\n",
    "        Translate the following English image caption into natural Korean.\n",
    "        If there are typos or abbreviations (e.g., 'assaultr'), fix them based on context before translating.\n",
    "        \n",
    "        English: \"{text}\"\n",
    "        Korean:\n",
    "        \"\"\"\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Gemini Ìò∏Ï∂ú Ïã§Ìå®: {e}\"\n",
    "\n",
    "# ==========================================\n",
    "# 3. ÏµúÏ¢Ö Í≤∞Í≥º ÌôïÏù∏\n",
    "# ==========================================\n",
    "print(\"\\n‚ú® === [Gemini(Pro)Í∞Ä Îã§Îì¨ÏùÄ ÏµúÏ¢Ö Í≤∞Í≥º] ===\")\n",
    "IMG_DIR = \"./data/train\"\n",
    "all_imgs = [os.path.join(dp, f) for dp, dn, fn in os.walk(IMG_DIR) for f in fn if f.endswith(('.jpg','.png'))]\n",
    "\n",
    "if all_imgs:\n",
    "    test_imgs = random.sample(all_imgs, min(3, len(all_imgs)))\n",
    "    \n",
    "    for img_path in test_imgs:\n",
    "        eng_cap = generate_vision_caption(img_path)\n",
    "        if eng_cap:\n",
    "            kor_cap = ask_gemini_translate(eng_cap)\n",
    "            \n",
    "            print(f\"\\nüñºÔ∏è ÌååÏùº: {os.path.basename(img_path)}\")\n",
    "            print(f\"üá∫üá∏ Vision AI: \\\"{eng_cap}\\\"\")\n",
    "            print(f\"üíé Gemini KO: \\\"{kor_cap}\\\"\")\n",
    "            print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"üö® Ïù¥ÎØ∏ÏßÄ Ìè¥ÎçîÍ∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41727a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• [Evaluation] CLIP Ï±ÑÏ†êÍ∏∞ Í∞ÄÎèô | Device: cuda\n",
      "üö® Ïò§Î•ò: './vit_gpt2_final_model' Ìè¥ÎçîÍ∞Ä ÏóÜÏäµÎãàÎã§. ÌïôÏäµÏù¥ ÏïÑÏßÅ Ïïà ÎÅùÎÇ¨ÎÇòÏöî?\n",
      "‚ö†Ô∏è ÌïôÏäµÎêú Î™®Îç∏ ÎåÄÏã† Í∏∞Î≥∏ Î™®Îç∏ÏùÑ Î°úÎìúÌïòÏó¨ ÌÖåÏä§Ìä∏Ìï©ÎãàÎã§.\n",
      "‚è≥ 1. ViT-GPT2(ÎÇ¥ Î™®Îç∏) Î°úÎìú Ï§ë...\n",
      "‚è≥ 2. CLIP(Ïã¨ÌåêÍ¥Ä) Î°úÎìú Ï§ë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä === [ÏµúÏ¢Ö Ï±ÑÏ†ê Î¶¨Ìè¨Ìä∏] ===\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 320.jpg\n",
      "ü§ñ AI Ï∫°ÏÖò: \"a close up picture of a black and white photo \"\n",
      "‚≠ê CLIP Ï†êÏàò: 19.71Ï†ê\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 296.jpg\n",
      "ü§ñ AI Ï∫°ÏÖò: \"a brown dog laying on a stone wall \"\n",
      "‚≠ê CLIP Ï†êÏàò: 22.80Ï†ê\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 269.jpg\n",
      "ü§ñ AI Ï∫°ÏÖò: \"a green and white striped bird sitting on top of a branch \"\n",
      "‚≠ê CLIP Ï†êÏàò: 22.42Ï†ê\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 412.jpg\n",
      "ü§ñ AI Ï∫°ÏÖò: \"a bottle of soda sitting on top of a green field \"\n",
      "‚≠ê CLIP Ï†êÏàò: 22.69Ï†ê\n",
      "\n",
      "üñºÔ∏è ÌååÏùº: 351.jpg\n",
      "ü§ñ AI Ï∫°ÏÖò: \"a brown and white dog laying on a wooden bench \"\n",
      "‚≠ê CLIP Ï†êÏàò: 28.37Ï†ê\n",
      "\n",
      "‚úÖ ÌèâÍ∑† Ï†êÏàò: 23.20Ï†ê\n",
      "üëç ÌõåÎ•≠Ìï©ÎãàÎã§! Î≤†Ïù¥Ïä§ÎùºÏù∏ÏùÑ Ìõ®Ïî¨ Îõ∞Ïñ¥ÎÑòÏóàÏäµÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# CLIP ÌèâÍ∞Ä ÏΩîÎìú (Ïó¨Í∏∞ÏÑúÎ∂ÄÌÑ∞ ÎÅùÍπåÏßÄ Î≥µÏÇ¨ÌïòÏÑ∏Ïöî)\n",
    "# ======================================================\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# 1. ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò/Î°úÎî©\n",
    "try:\n",
    "    import clip\n",
    "    from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "except ImportError:\n",
    "    print(\"üì¶ ÌïÑÏàò ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏóÜÏñ¥ÏÑú ÏÑ§ÏπòÌï©ÎãàÎã§...\")\n",
    "    !pip install git+https://github.com/openai/CLIP.git transformers\n",
    "    import clip\n",
    "    from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "# 2. Í≤ΩÎ°ú ÏÑ§Ï†ï (ÌïôÏäµÎêú Î™®Îç∏ ÏúÑÏπò)\n",
    "# ------------------------------------------------------\n",
    "MODEL_PATH = \"./vit_gpt2_final_model\"   # Î∞©Í∏à ÌïôÏäµ ÎÅùÎÇú Î™®Îç∏ Ìè¥Îçî\n",
    "IMG_DIR = \"./data/train\"                # Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî\n",
    "# ------------------------------------------------------\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üî• [Evaluation] CLIP Ï±ÑÏ†êÍ∏∞ Í∞ÄÎèô | Device: {device}\")\n",
    "\n",
    "# 3. Î™®Îç∏ Î°úÎìú Í≤ÄÏ¶ù\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"üö® Ïò§Î•ò: '{MODEL_PATH}' Ìè¥ÎçîÍ∞Ä ÏóÜÏäµÎãàÎã§. ÌïôÏäµÏù¥ ÏïÑÏßÅ Ïïà ÎÅùÎÇ¨ÎÇòÏöî?\")\n",
    "    # ÌïôÏäµÏù¥ Ïïà ÎÅùÎÇ¨ÏúºÎ©¥ Í∏∞Î≥∏ Î™®Îç∏Ïù¥ÎùºÎèÑ Î°úÎìúÌï¥ÏÑú ÌÖåÏä§Ìä∏ (ÏûÑÏãú)\n",
    "    print(\"‚ö†Ô∏è ÌïôÏäµÎêú Î™®Îç∏ ÎåÄÏã† Í∏∞Î≥∏ Î™®Îç∏ÏùÑ Î°úÎìúÌïòÏó¨ ÌÖåÏä§Ìä∏Ìï©ÎãàÎã§.\")\n",
    "    MODEL_PATH = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "print(\"‚è≥ 1. ViT-GPT2(ÎÇ¥ Î™®Îç∏) Î°úÎìú Ï§ë...\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "print(\"‚è≥ 2. CLIP(Ïã¨ÌåêÍ¥Ä) Î°úÎìú Ï§ë...\")\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# 4. ÌèâÍ∞Ä Ìï®Ïàò\n",
    "def generate_caption(image_path):\n",
    "    try:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        pixel_values = feature_extractor(images=[i_image], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        \n",
    "        # Îπî ÏÑúÏπò Ï†ÅÏö© (Îçî ÎòëÎòëÌïú Î¨∏Ïû• ÏÉùÏÑ±)\n",
    "        output_ids = model.generate(\n",
    "            pixel_values, \n",
    "            max_length=50, \n",
    "            num_beams=4, \n",
    "            early_stopping=True\n",
    "        )\n",
    "        caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return caption, i_image\n",
    "    except Exception as e:\n",
    "        return None, None\n",
    "\n",
    "def get_clip_score(image, caption):\n",
    "    image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "    text_input = clip.tokenize([caption[:76]]).to(device) # Í∏∏Ïù¥ Ï†úÌïú Ï≤òÎ¶¨\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "        \n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (image_features @ text_features.T).item()\n",
    "        \n",
    "    return similarity * 100\n",
    "\n",
    "# 5. Ïã§Ìñâ Î∞è Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(\"\\nüìä === [ÏµúÏ¢Ö Ï±ÑÏ†ê Î¶¨Ìè¨Ìä∏] ===\")\n",
    "all_images = []\n",
    "for root, dirs, files in os.walk(IMG_DIR):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            all_images.append(os.path.join(root, file))\n",
    "\n",
    "if not all_images:\n",
    "    print(\"üö® ÌÖåÏä§Ìä∏Ìï† Ïù¥ÎØ∏ÏßÄÍ∞Ä Ìè¥ÎçîÏóê ÏóÜÏäµÎãàÎã§!\")\n",
    "else:\n",
    "    # ÎûúÎç§ 5Ïû• ÎΩëÍ∏∞\n",
    "    test_images = random.sample(all_images, min(5, len(all_images)))\n",
    "    total_score = 0\n",
    "    valid_count = 0\n",
    "\n",
    "    for img_path in test_images:\n",
    "        caption, image = generate_caption(img_path)\n",
    "        if caption:\n",
    "            score = get_clip_score(image, caption)\n",
    "            total_score += score\n",
    "            valid_count += 1\n",
    "            \n",
    "            print(f\"\\nüñºÔ∏è ÌååÏùº: {os.path.basename(img_path)}\")\n",
    "            print(f\"ü§ñ AI Ï∫°ÏÖò: \\\"{caption}\\\"\")\n",
    "            print(f\"‚≠ê CLIP Ï†êÏàò: {score:.2f}Ï†ê\")\n",
    "\n",
    "    if valid_count > 0:\n",
    "        avg = total_score / valid_count\n",
    "        print(f\"\\n‚úÖ ÌèâÍ∑† Ï†êÏàò: {avg:.2f}Ï†ê\")\n",
    "        if avg > 25:\n",
    "            print(\"üéâ Ï∂ïÌïòÌï©ÎãàÎã§! SOTAÍ∏â ÏÑ±Îä•ÏûÖÎãàÎã§. (ÏÇ¨ÎûåÍ≥º ÎπÑÏä∑Ìïú ÏàòÏ§Ä)\")\n",
    "        elif avg > 20:\n",
    "            print(\"üëç ÌõåÎ•≠Ìï©ÎãàÎã§! Î≤†Ïù¥Ïä§ÎùºÏù∏ÏùÑ Ìõ®Ïî¨ Îõ∞Ïñ¥ÎÑòÏóàÏäµÎãàÎã§.\")\n",
    "        else:\n",
    "            print(\"üí™ Ï°∞Í∏à Îçî ÌïôÏäµÏù¥ ÌïÑÏöîÌï©ÎãàÎã§. (EpochÎ•º ÎäòÎ†§Î≥¥ÏÑ∏Ïöî)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ce9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è ÏµúÏ¢Ö Î™®Îç∏ ÎåÄÏã† 'Ï§ëÍ∞Ñ Ï†ÄÏû• Î™®Îç∏'ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§: ./vit_gpt2_result\\checkpoint-2710\n",
      "üî• Ï±ÑÏ†ê ÏãúÏûë! (ÏÇ¨Ïö© Î™®Îç∏: ./vit_gpt2_result\\checkpoint-2710)\n",
      "‚ùå Î™®Îç∏ Î°úÎìú Ï§ë ÏóêÎü¨ Î∞úÏÉù: <class 'transformers.models.vision_encoder_decoder.configuration_vision_encoder_decoder.VisionEncoderDecoderConfig'>\n",
      "\n",
      "üìä === [ÏßÑÏßú ÏÑ±Ï†ÅÌëú] ===\n",
      "üñºÔ∏è 437.jpg | ü§ñ  blackanot a white standing the near the | ‚≠ê 25.49Ï†ê\n",
      "üñºÔ∏è 133.jpg | ü§ñ  bright americ chleon standing a green | ‚≠ê 22.95Ï†ê\n",
      "üñºÔ∏è 220.jpg | ü§ñ  whiteship a white with large oval floating the | ‚≠ê 24.95Ï†ê\n",
      "üñºÔ∏è 193.jpg | ü§ñ  americ co with red and shell on stone | ‚≠ê 15.37Ï†ê\n",
      "üñºÔ∏è 706.jpg | ü§ñ  red whitear with spots fallen | ‚≠ê 25.90Ï†ê\n",
      "\n",
      "‚úÖ ÏµúÏ¢Ö ÌèâÍ∑† Ï†êÏàò: 22.93Ï†ê\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "import random\n",
    "try:\n",
    "    import clip\n",
    "    from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "except ImportError:\n",
    "    !pip install git+https://github.com/openai/CLIP.git transformers\n",
    "    import clip\n",
    "    from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "# ==========================================\n",
    "# 1. Î™®Îç∏ ÏúÑÏπò ÏûêÎèô ÌÉêÏÉâ (ÌÉêÏßÄÍ≤¨ Î™®Îìú üê∂)\n",
    "# ==========================================\n",
    "# Ïö∞ÏÑ†ÏàúÏúÑ 1: ÏµúÏ¢Ö Ï†ÄÏû•Îêú Î™®Îç∏\n",
    "final_path = \"./vit_gpt2_final_model\"\n",
    "# Ïö∞ÏÑ†ÏàúÏúÑ 2: ÌïôÏäµ ÎèÑÏ§ë Ï†ÄÏû•Îêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ (Í∞ÄÏû• ÏµúÏã† Í≤É)\n",
    "ckpt_path = \"./vit_gpt2_result\"\n",
    "\n",
    "model_to_use = \"\"\n",
    "\n",
    "if os.path.exists(final_path) and len(os.listdir(final_path)) > 0:\n",
    "    print(f\"‚úÖ ÏµúÏ¢Ö Î™®Îç∏ Î∞úÍ≤¨! ({final_path})\")\n",
    "    model_to_use = final_path\n",
    "elif os.path.exists(ckpt_path):\n",
    "    # Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ìè¥ÎçîÎì§ Ï§ë Í∞ÄÏû• Ïà´ÏûêÍ∞Ä ÌÅ∞(ÏµúÏã†) Í≤É Ï∞æÍ∏∞\n",
    "    ckpts = glob.glob(os.path.join(ckpt_path, \"checkpoint-*\"))\n",
    "    if len(ckpts) > 0:\n",
    "        latest_ckpt = max(ckpts, key=os.path.getctime) # Í∞ÄÏû• ÏµúÍ∑ºÏóê ÏàòÏ†ïÎêú Ìè¥Îçî\n",
    "        print(f\"‚ö†Ô∏è ÏµúÏ¢Ö Î™®Îç∏ ÎåÄÏã† 'Ï§ëÍ∞Ñ Ï†ÄÏû• Î™®Îç∏'ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§: {latest_ckpt}\")\n",
    "        model_to_use = latest_ckpt\n",
    "    else:\n",
    "        print(\"üö® Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ìè¥ÎçîÎäî ÏûàÎäîÎç∞ ÎÇ¥Ïö©Î¨ºÏù¥ ÌÖÖ ÎπÑÏóàÏäµÎãàÎã§.\")\n",
    "else:\n",
    "    print(\"üö® [ÎπÑÏÉÅ] ÌïôÏäµÎêú Î™®Îç∏ Ìè¥ÎçîÍ∞Ä ÏïÑÏòà Ïïà Î≥¥ÏûÖÎãàÎã§!\")\n",
    "\n",
    "# Î™®Îç∏Ïù¥ ÏóÜÏúºÎ©¥ Í∏∞Î≥∏ Î™®Îç∏ ÏÇ¨Ïö© (ÎπÑÍµêÏö©)\n",
    "if not model_to_use:\n",
    "    print(\"üò≠ Ïñ¥Ï©î Ïàò ÏóÜÏù¥ 'Í∏∞Î≥∏(Base) Î™®Îç∏'Î°ú ÌÖåÏä§Ìä∏Ìï©ÎãàÎã§.\")\n",
    "    model_to_use = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. Î™®Îç∏ Î°úÎìú Î∞è Ï±ÑÏ†ê\n",
    "# ==========================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üî• Ï±ÑÏ†ê ÏãúÏûë! (ÏÇ¨Ïö© Î™®Îç∏: {model_to_use})\")\n",
    "\n",
    "try:\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(model_to_use).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "    feature_extractor = ViTImageProcessor.from_pretrained(model_to_use)\n",
    "    \n",
    "    # CLIP Î°úÎìú\n",
    "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Î™®Îç∏ Î°úÎìú Ï§ë ÏóêÎü¨ Î∞úÏÉù: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ÌèâÍ∞Ä Ìï®Ïàò\n",
    "def generate_caption(image_path):\n",
    "    try:\n",
    "        i_image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = feature_extractor(images=[i_image], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        output_ids = model.generate(pixel_values, max_length=50, num_beams=4, early_stopping=True)\n",
    "        caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return caption, i_image\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def get_clip_score(image, caption):\n",
    "    image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "    text_input = clip.tokenize([caption[:76]]).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_f = clip_model.encode_image(image_input)\n",
    "        txt_f = clip_model.encode_text(text_input)\n",
    "        score = (img_f / img_f.norm(dim=-1, keepdim=True) @ (txt_f / txt_f.norm(dim=-1, keepdim=True)).T).item()\n",
    "    return score * 100\n",
    "\n",
    "# ==========================================\n",
    "# 3. Í≤∞Í≥º Ï∂úÎ†•\n",
    "# ==========================================\n",
    "IMG_DIR = \"./data/train\"\n",
    "all_imgs = [os.path.join(dp, f) for dp, dn, fn in os.walk(IMG_DIR) for f in fn if f.endswith(('.jpg','.png'))]\n",
    "\n",
    "if all_imgs:\n",
    "    test_imgs = random.sample(all_imgs, min(5, len(all_imgs)))\n",
    "    total_score = 0\n",
    "    print(\"\\nüìä === [ÏßÑÏßú ÏÑ±Ï†ÅÌëú] ===\")\n",
    "    for img_path in test_imgs:\n",
    "        cap, img = generate_caption(img_path)\n",
    "        if cap:\n",
    "            score = get_clip_score(img, cap)\n",
    "            total_score += score\n",
    "            print(f\"üñºÔ∏è {os.path.basename(img_path)} | ü§ñ {cap} | ‚≠ê {score:.2f}Ï†ê\")\n",
    "    \n",
    "    avg = total_score / len(test_imgs)\n",
    "    print(f\"\\n‚úÖ ÏµúÏ¢Ö ÌèâÍ∑† Ï†êÏàò: {avg:.2f}Ï†ê\")\n",
    "    if \"nlpconnect\" not in model_to_use and avg > 23.2:\n",
    "        print(\"üöÄ ÏÑ±Í≥µ! Í∏∞Î≥∏ Î™®Îç∏Î≥¥Îã§ Ï†êÏàòÍ∞Ä ÎÜíÏäµÎãàÎã§. ÌïôÏäµ Ìö®Í≥ºÍ∞Ä ÏûàÏäµÎãàÎã§!\")\n",
    "    elif \"nlpconnect\" in model_to_use:\n",
    "        print(\"‚ÑπÔ∏è Ïù¥Í±¥ Í∏∞Î≥∏ Î™®Îç∏ Ï†êÏàòÏûÖÎãàÎã§. (ÎπÑÍµê Í∏∞Ï§Ä)\")\n",
    "else:\n",
    "    print(\"üö® Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî Í≤ΩÎ°úÎ•º Î™ª Ï∞æÍ≤†ÏäµÎãàÎã§.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dd389f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Î≤àÏó≠Í∏∞ ÎùºÏù¥Î∏åÎü¨Î¶¨(sentencepiece) ÏÑ§Ïπò Ï§ë...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Users\\ÌòÑÏÑùÏù¥\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 8.4 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "üî• [System] ÌïúÏòÅ ÌÜµÌï© ÏãúÏä§ÌÖú Í∞ÄÎèô | Device: cuda\n",
      "üìÇ Ïó∞Í≤∞Îêú Ï∫°ÏÖò Î™®Îç∏: ./vit_gpt2_result\\checkpoint-813\n",
      "‚è≥ 1. Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Î™®Îç∏(ViT-GPT2) Î°úÎìú Ï§ë...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "<class 'transformers.models.vision_encoder_decoder.configuration_vision_encoder_decoder.VisionEncoderDecoderConfig'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚è≥ 1. Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Î™®Îç∏(ViT-GPT2) Î°úÎìú Ï§ë...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m feature_extractor = ViTImageProcessor.from_pretrained(MODEL_PATH)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# (2) Î≤àÏó≠ Î™®Îç∏ (Helsinki-NLP) - CPU/GPU Ïú†ÎèôÏ†Å ÏÇ¨Ïö©\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# GTX 1070 Î©îÎ™®Î¶¨ Ï†àÏïΩÏùÑ ÏúÑÌï¥ Î≤àÏó≠Í∏∞Îäî CPUÎ°ú ÎèåÎ¶¨Îäî Í≤ÉÎèÑ Î∞©Î≤ïÏûÖÎãàÎã§.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ÌòÑÏÑùÏù¥\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1172\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1170\u001b[39m model_type = config_class_to_model_type(\u001b[38;5;28mtype\u001b[39m(config).\u001b[34m__name__\u001b[39m)\n\u001b[32m   1171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1172\u001b[39m     tokenizer_class_py, tokenizer_class_fast = \u001b[43mTOKENIZER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1175\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ÌòÑÏÑùÏù¥\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:815\u001b[39m, in \u001b[36m_LazyAutoMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    813\u001b[39m         model_name = \u001b[38;5;28mself\u001b[39m._model_mapping[mtype]\n\u001b[32m    814\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._load_attr_from_module(mtype, model_name)\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: <class 'transformers.models.vision_encoder_decoder.configuration_vision_encoder_decoder.VisionEncoderDecoderConfig'>"
     ]
    }
   ],
   "source": [
    "# 1. ÌïÑÏàò ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÑ§Ïπò (Î≤àÏó≠Í∏∞Ïö©)\n",
    "try:\n",
    "    import sentencepiece\n",
    "    from transformers import MarianMTModel, MarianTokenizer\n",
    "except ImportError:\n",
    "    print(\"üì¶ Î≤àÏó≠Í∏∞ ÎùºÏù¥Î∏åÎü¨Î¶¨(sentencepiece) ÏÑ§Ïπò Ï§ë...\")\n",
    "    !pip install sentencepiece\n",
    "    from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import os\n",
    "import random\n",
    "\n",
    "# ==========================================\n",
    "# 1. ÏÑ§Ï†ï (Î™®Îç∏ Í≤ΩÎ°ú)\n",
    "# ==========================================\n",
    "# ÌïôÏäµÎêú Î™®Îç∏ Ï∞æÍ∏∞\n",
    "if os.path.exists(\"./vit_gpt2_final_model\"):\n",
    "    MODEL_PATH = \"./vit_gpt2_final_model\"\n",
    "elif os.path.exists(\"./vit_gpt2_result\"):\n",
    "    import glob\n",
    "    ckpts = glob.glob(\"./vit_gpt2_result/checkpoint-*\")\n",
    "    MODEL_PATH = max(ckpts, key=os.path.getctime) if ckpts else \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "else:\n",
    "    MODEL_PATH = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "\n",
    "IMG_DIR = \"./data/train\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"üî• [System] ÌïúÏòÅ ÌÜµÌï© ÏãúÏä§ÌÖú Í∞ÄÎèô | Device: {device}\")\n",
    "print(f\"üìÇ Ïó∞Í≤∞Îêú Ï∫°ÏÖò Î™®Îç∏: {MODEL_PATH}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Î™®Îç∏ Î°úÎìú (Ï∫°ÏÖò ÏÉùÏÑ±Í∏∞ + Î≤àÏó≠Í∏∞)\n",
    "# ==========================================\n",
    "# (1) Ï∫°ÏÖò Î™®Îç∏ (ViT-GPT2) - GPU ÏÇ¨Ïö©\n",
    "print(\"‚è≥ 1. Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Î™®Îç∏(ViT-GPT2) Î°úÎìú Ï§ë...\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# (2) Î≤àÏó≠ Î™®Îç∏ (Helsinki-NLP) - CPU/GPU Ïú†ÎèôÏ†Å ÏÇ¨Ïö©\n",
    "# GTX 1070 Î©îÎ™®Î¶¨ Ï†àÏïΩÏùÑ ÏúÑÌï¥ Î≤àÏó≠Í∏∞Îäî CPUÎ°ú ÎèåÎ¶¨Îäî Í≤ÉÎèÑ Î∞©Î≤ïÏûÖÎãàÎã§.\n",
    "print(\"‚è≥ 2. ÌïúÏòÅ Î≤àÏó≠Í∏∞(Translator) Î°úÎìú Ï§ë...\")\n",
    "trans_model_name = \"Helsinki-NLP/opus-mt-en-ko\"\n",
    "trans_tokenizer = MarianTokenizer.from_pretrained(trans_model_name)\n",
    "trans_model = MarianMTModel.from_pretrained(trans_model_name).to(device) # Î©îÎ™®Î¶¨ Î∂ÄÏ°±ÌïòÎ©¥ .to(\"cpu\")Î°ú Î≥ÄÍ≤Ω\n",
    "\n",
    "# ==========================================\n",
    "# 3. Í∏∞Îä• Ìï®Ïàò Ï†ïÏùò\n",
    "# ==========================================\n",
    "def generate_caption(image_path):\n",
    "    try:\n",
    "        i_image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = feature_extractor(images=[i_image], return_tensors=\"pt\").pixel_values.to(device)\n",
    "        \n",
    "        output_ids = model.generate(pixel_values, max_length=50, num_beams=4, early_stopping=True)\n",
    "        eng_caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return eng_caption, i_image\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def translate_to_korean(text):\n",
    "    # Î≤àÏó≠Í∏∞Í∞Ä ÏïåÏïÑÎì§ÏùÑ Ïàò ÏûàÍ≤å ÌÜ†ÌÅ∞Ìôî\n",
    "    inputs = trans_tokenizer(text, return_tensors=\"pt\", padding=True).to(device) # Î©îÎ™®Î¶¨ Î∂ÄÏ°±ÌïòÎ©¥ .to(\"cpu\")\n",
    "    \n",
    "    # Î≤àÏó≠ ÏÉùÏÑ±\n",
    "    translated = trans_model.generate(**inputs)\n",
    "    kor_text = trans_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return kor_text\n",
    "\n",
    "# ==========================================\n",
    "# 4. Ïã§Ìñâ Î∞è Í≤∞Í≥º Ï∂úÎ†•\n",
    "# ==========================================\n",
    "print(\"\\n‚ú® === [AI Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù Î∞è Î≤àÏó≠ Í≤∞Í≥º] ===\")\n",
    "all_imgs = [os.path.join(dp, f) for dp, dn, fn in os.walk(IMG_DIR) for f in fn if f.endswith(('.jpg','.png'))]\n",
    "\n",
    "if all_imgs:\n",
    "    test_imgs = random.sample(all_imgs, min(3, len(all_imgs))) # 3Ïû•Îßå ÌÖåÏä§Ìä∏\n",
    "    \n",
    "    for img_path in test_imgs:\n",
    "        eng_cap, img = generate_caption(img_path)\n",
    "        if eng_cap:\n",
    "            # Ïó¨Í∏∞ÏÑú Î≤àÏó≠ Ïã§Ìñâ!\n",
    "            kor_cap = translate_to_korean(eng_cap)\n",
    "            \n",
    "            print(f\"\\nüñºÔ∏è ÌååÏùº: {os.path.basename(img_path)}\")\n",
    "            print(f\"üá∫üá∏ ÏòÅÏñ¥(Vision): {eng_cap}\")\n",
    "            print(f\"üá∞üá∑ ÌïúÍµ≠Ïñ¥(Trans): {kor_cap}\")\n",
    "            print(\"-\" * 30)\n",
    "else:\n",
    "    print(\"üö® Ïù¥ÎØ∏ÏßÄ Ìè¥ÎçîÍ∞Ä ÎπÑÏñ¥ÏûàÏäµÎãàÎã§.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
