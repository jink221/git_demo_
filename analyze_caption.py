import json
import re
from collections import Counter, defaultdict
from pathlib import Path

# ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
JSON_PATH = Path("./data/captions/train.json")

def analyze_caption_quality(json_path):
    if not json_path.exists():
        print(f"[ERROR] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
        return

    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    total = len(data)
    if total == 0:
        print("ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    word_counts = []
    class_stats = defaultdict(lambda: {"total": 0, "issues": 0})
    
    # ê²°í•¨ ìœ í˜• í™•ì¥
    defects = {
        "repeated_words": 0,    # ë™ì¼ ë‹¨ì–´ ì—°ì† ë°˜ë³µ
        "logical_errors": 0,    # and or, ì´ì¤‘ ê´€ì‚¬ ë“±
        "no_verb_structure": 0, # ë™ì‚¬/ì „ì¹˜ì‚¬ ë¶€ì¬
        "too_short": 0,         # ë‹¨ì–´ ìˆ˜ ë¶€ì¡±
        "class_name_missing": 0, # [ì¶”ê°€] ìº¡ì…˜ì— í´ë˜ìŠ¤ëª…ì´ ì—†ìŒ
        "junk_contained": 0     # [ì¶”ê°€] Photo by, Getty ë“± í¬í•¨
    }

    verb_keywords = ["is", "are", "sitting", "standing", "flying", "on", "in", "with", "eating", "perched"]
    junk_keywords = ["photo by", "getty", "images", "stock", "copyright", "ltd", "available"]

    for item in data:
        cap = item.get('caption', '').strip()
        c_name = item.get('class', 'unknown').replace('_', ' ').lower()
        low_cap = cap.lower().replace('.', '')
        words = low_cap.split()
        
        class_stats[c_name]["total"] += 1
        has_issue = False
        current_issues = []

        # 1. ê¸¸ì´ ì²´í¬
        word_counts.append(len(words))
        if len(words) < 5:
            defects["too_short"] += 1
            has_issue = True

        # 2. ì •í¬ ë¬¸êµ¬ ì²´í¬ (BLIP íŠ¹í™” ê²°í•¨)
        if any(j in low_cap for j in junk_keywords):
            defects["junk_contained"] += 1
            has_issue = True

        # 3. í´ë˜ìŠ¤ëª… ëˆ„ë½ ì²´í¬
        if c_name not in low_cap:
            defects["class_name_missing"] += 1
            has_issue = True

        # 4. ë…¼ë¦¬/ë¬¸ë²• ì˜¤ë¥˜ (ì´ì¤‘ ê´€ì‚¬ ì¶”ê°€)
        if re.search(r"\b(and|or)\s+(and|or)\b", low_cap) or ", ," in low_cap or re.search(r"\b(a|an|the)\s+(a|an|the)\b", low_cap):
            defects["logical_errors"] += 1
            has_issue = True

        # 5. ì—°ì† ë‹¨ì–´ ë°˜ë³µ
        if any(words[i] == words[i+1] for i in range(len(words)-1)):
            defects["repeated_words"] += 1
            has_issue = True

        # 6. ë™ì‚¬ êµ¬ì¡° ì²´í¬
        if not any(v in low_cap for v in verb_keywords):
            defects["no_verb_structure"] += 1
            has_issue = True
            
        if has_issue:
            class_stats[c_name]["issues"] += 1

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n" + "="*60)
    print(f"ğŸ“Š ìº¡ì…˜ í’ˆì§ˆ ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸: {json_path.name}")
    print("="*60)
    print(f"âœ… ì „ì²´ ì´ë¯¸ì§€ ìˆ˜: {total:10}")
    print(f"ğŸ“ í‰ê·  ë‹¨ì–´ ê¸¸ì´: {sum(word_counts)/total:10.2f} ë‹¨ì–´")
    print("-" * 60)
    
    print(f"âš ï¸ [ìœ í˜•ë³„ ê²°í•¨ ë¬¸ì¥ ë¹„ìœ¨]")
    for k, v in defects.items():
        percentage = (v / total) * 100
        print(f"   - {k:20}: {v:5} ê±´ ({percentage:5.1f}%)")
        
    print("-" * 60)
    print(f"ğŸš¨ [ê²°í•¨ë¥ ì´ ë†’ì€ TOP 5 í´ë˜ìŠ¤]")
    worst_classes = sorted(
        class_stats.items(), 
        key=lambda x: x[1]["issues"] / x[1]["total"] if x[1]["total"] > 0 else 0, 
        reverse=True
    )[:5]
    
    for name, stat in worst_classes:
        fail_rate = (stat["issues"] / stat["total"]) * 100
        print(f"   - {name:22}: {fail_rate:5.1f}% ({stat['issues']}/{stat['total']})")
    
    print("=" * 60)
    print("ğŸ’¡ junk_containedê°€ ë†’ìœ¼ë©´: normalize_captionì˜ JUNK_PATTERNSë¥¼ ë³´ê°•í•˜ì„¸ìš”.")
    print("ğŸ’¡ class_name_missingì´ ë†’ìœ¼ë©´: í”„ë¡¬í”„íŠ¸ì— í´ë˜ìŠ¤ëª…ì„ ë” ëª…í™•íˆ ì£¼ì…í•˜ì„¸ìš”.\n")

if __name__ == "__main__":
    analyze_caption_quality(JSON_PATH)